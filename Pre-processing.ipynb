{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to read the text from the MIMIC dataset, concatenate all the notes, store it as a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"NOTEEVENTS.csv\")\n",
    "\n",
    "df.head()\n",
    "\n",
    "tex = \"\".join(df.iloc[:,-1])\n",
    "\n",
    "tex = tex.replace(\"\\n\",\" \")\n",
    "\n",
    "file = open(\"notes.txt\",\"w\") \n",
    "\n",
    "file.write(tex)  \n",
    "\n",
    "file.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file and extract a 500 million character subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"notes.txt\",\"r\") \n",
    "text = file.read()\n",
    "\n",
    "file.close() \n",
    "\n",
    "text = text[:5000000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locate the PHI in the text, and make a separate string where PHI caracters are replaced by $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = \"\"\n",
    "phi = False\n",
    "i = 0\n",
    "while i < len(text):\n",
    "    if i + 3 >= len(text):\n",
    "        temp+=text[i:]\n",
    "        break\n",
    "    snip = text[i:i+3]\n",
    "    if snip == \"[**\":\n",
    "        phi = True\n",
    "    if phi:\n",
    "        if snip == \"**]\":\n",
    "            temp+=\"$$$\"\n",
    "            phi = False\n",
    "            i+=3\n",
    "        else:\n",
    "            temp+=\"$\"\n",
    "            i+=1\n",
    "        continue\n",
    "    temp+=text[i]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the PHI reference string created in the previous step to separate PHI and no-PHI.\n",
    "The result is a list where each element is a string, which is either all PHI nor non-PHI. At the same time we create a labels list which labels each element as 1 (phi) or 0 (non-phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"separating phi and non phi\")\n",
    "step = 1\n",
    "all_listed = []\n",
    "i = 0\n",
    "labels = []\n",
    "while i < len(text)-1:\n",
    "    temp_phi_str = \"\"\n",
    "    temp_other_str = \"\"\n",
    "    if temp[i] == \"$\":\n",
    "        for j in range(i,len(text)):\n",
    "            if temp[j] == \"$\":\n",
    "                temp_phi_str+=text[j]\n",
    "            else:\n",
    "                break\n",
    "        step = len(temp_phi_str)\n",
    "        i+=step\n",
    "        all_listed.append(temp_phi_str)\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        for j in range(i,len(text)):\n",
    "            if temp[j] != \"$\":\n",
    "                temp_other_str += text[j]\n",
    "            else:\n",
    "                break\n",
    "        step = len(temp_other_str)\n",
    "        i+=step\n",
    "        all_listed.append(temp_other_str)\n",
    "        labels.append(0)\n",
    "    step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the elements of the list created in the previous step is split on whitespace and stripped of trailing and leading whitespaces.\n",
    "The resulting words are again appendied to another list in order, and the corresponding labels are expanded appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_labels_list = []\n",
    "split_word_list = []\n",
    "    \n",
    "for i in range(len(all_listed)):\n",
    "    if labels[i] == 0:\n",
    "        all_listed[i] = \" \".join(all_listed[i].split())\n",
    "        temp_split_list = all_listed[i].split(\" \")\n",
    "        split_word_list.extend(temp_split_list)\n",
    "        split_labels_list.extend([labels[i]]*len(temp_split_list))\n",
    "    else:\n",
    "        split_word_list.append(all_listed[i][3:-3])\n",
    "        split_labels_list.append(labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tests/rules, based on which the labels are expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isdate(string):\n",
    "    pattern = re.compile(\"([0-9]|[0-9]{2}|[0-9]{4})-([0-9]|[0-9]{2}|[0-9]{4})-?([0-9]|[0-9]{2}|[0-9]{4})?$\")\n",
    "    return bool(re.match(pattern,string))\n",
    "\n",
    "\n",
    "#def isage(string):\n",
    "#    if string in age_phi_lis:\n",
    "#        return True\n",
    "#    else:\n",
    "#        return False\n",
    "    \n",
    "    \n",
    "def get_finer_labels(orig_string,orig_label):\n",
    "    label = orig_label\n",
    "    string = orig_string.lower()\n",
    "    string = \" \".join(string.split())\n",
    "    if \"name\" in string:\n",
    "        label = \"NAME\"\n",
    "    elif \"location\" in string or \"apart\" in string or \"address\" in string or \"country\" in string:\n",
    "        label= \"LOC\"\n",
    "    elif \"hospital\" in string:\n",
    "        label= \"HOSP\"\n",
    "    elif isdate(string):\n",
    "        label = \"DATE\"\n",
    "    elif \"Month/Day\" in orig_string:\n",
    "        label = \"MNTH/DAY\"\n",
    "    elif \"telephone\" in string or \"fax\" in string:\n",
    "        label = \"CONTACT\"\n",
    "    elif \"number\" in string or \"identif\"in string :\n",
    "        label = \"UNIQID\"\n",
    "    #elif isage(orig_string):\n",
    "    #    label = \"AGE\"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual expansion of labels is done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finer_labels = []\n",
    "for i in range(len(split_word_list)):\n",
    "    finer_labels.append(get_finer_labels(split_word_list[i],split_labels_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-identification of data i.e. replacing vague place holder with realistic lookoing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random names, addresses and hospiytals are stored in these files\n",
    "file = open(\"first_names_female.txt\",\"r\") \n",
    "first_names_f = (file.read()).split(\"\\n\")  \n",
    "file.close()\n",
    "\n",
    "file = open(\"first_names_male.txt\",\"r\") \n",
    "first_names_m = (file.read()).split(\"\\n\")  \n",
    "file.close()\n",
    "\n",
    "file = open(\"last_names.txt\",\"r\") \n",
    "last_names = (file.read()).split(\"\\n\")  \n",
    "file.close()\n",
    "\n",
    "file = open(\"sampled_hospitals.txt\",\"r\") \n",
    "hospitals = (file.read()).split(\"\\n\")  \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Foe dates\n",
    "def get_random_date():\n",
    "    month = str(random.sample(range(1,12),1)[0])\n",
    "    if month in [1,3,5,7,8,10,12]:\n",
    "        day = str(random.sample(range(1,31),1)[0])\n",
    "    elif month==2:\n",
    "        day = str(random.sample(range(1,28),1)[0])\n",
    "    else:\n",
    "        day = str(random.sample(range(1,30),1)[0])\n",
    "    return month+\"-\"+day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hospitals\n",
    "def get_random_hosp():\n",
    "    return random.choice(hospitals)\n",
    "\n",
    "#names\n",
    "def get_random_name(string):\n",
    "    string = string.lower()\n",
    "    first = random.choice([random.choice(first_names_m),random.choice(first_names_f)])\n",
    "    last = random.choice(last_names)\n",
    "    if \"first\" in string:\n",
    "        return first\n",
    "    elif \"last\" in string:\n",
    "        return last\n",
    "    else:\n",
    "        return first + \" \" + last\n",
    "\n",
    "    \n",
    "from random import randint\n",
    "\n",
    "#random numbers\n",
    "def random_with_N_digits(n):\n",
    "    range_start = 10**(n-1)\n",
    "    range_end = (10**n)-1\n",
    "    return randint(range_start, range_end)\n",
    "\n",
    "    \n",
    "from numpy.random import choice\n",
    "def get_random_phone_fax_number():\n",
    "    nums = [str(random_with_N_digits(3))+\"-\"+str(random_with_N_digits(3))+\"-\"+str(random_with_N_digits(4)) for x in range(10)]\n",
    "    nums.append(str(random_with_N_digits(10)))\n",
    "    return random.choice(nums)\n",
    "\n",
    "#unique ids\n",
    "def get_uniqid():\n",
    "    nums = [str(random_with_N_digits(x)) for x in [4,6,10,12,16]]\n",
    "    return random.choice(nums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual filling in of data is done here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"filling in reid\")\n",
    "\n",
    "for i in tqdm(range(len(split_word_list))):\n",
    "    if finer_labels[i] == \"MNTH/DAY\":\n",
    "        rand_date = get_random_date()\n",
    "        finer_labels[i] = \"DATE\"\n",
    "        split_word_list[i] = rand_date\n",
    "    elif finer_labels[i] == \"HOSP\":\n",
    "        rand_hosp = get_random_hosp()\n",
    "        split_word_list[i] = rand_hosp\n",
    "        finer_labels[i] = \"LOC\"\n",
    "    elif finer_labels[i] == \"NAME\":\n",
    "        split_word_list[i] = get_random_name(split_word_list[i])\n",
    "    elif finer_labels[i] == \"CONTACT\":\n",
    "        split_word_list[i] = get_random_phone_fax_number()\n",
    "    elif finer_labels[i] == \"UNIQID\":\n",
    "        split_word_list[i] = get_uniqid()\n",
    "    elif finer_labels[i] == \"AGE\":\n",
    "        split_word_list[i] = str(random.choice(range(2,90)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the PHI labels are replaced by finer labels, we label everything non-PHI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(split_word_list)):\n",
    "    split_word_list[i] = \" \".join(split_word_list[i].split()).lower()\n",
    "    if finer_labels[i] == 1 or finer_labels[i] == 0:\n",
    "        finer_labels[i] = \"OTHER\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK word tokenize, we tokenize phi and non-phi string while expanding the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word_list = []\n",
    "tokenized_label_list = []\n",
    "for i in tqdm(range(len(split_word_list))):\n",
    "        temp_split_list = word_tokenize(split_word_list[i])\n",
    "        tokenized_word_list.extend(temp_split_list)\n",
    "        tokenized_label_list.extend([finer_labels[i]]*len(temp_split_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To take care of date data, because dates are split by word tokeinze on hyphens which are then removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word_list2 = []\n",
    "tokenized_label_list2 = []\n",
    "for i in tqdm(range(len(tokenized_word_list))):\n",
    "        temp_split_list = tokenized_word_list[i].split(\"-\")\n",
    "        if len(temp_split_list) == 1:\n",
    "            tokenized_word_list2.append(tokenized_word_list[i])\n",
    "            tokenized_label_list2.append(tokenized_label_list[i])\n",
    "        elif len(temp_split_list) == 2:\n",
    "            tokenized_word_list2.extend([temp_split_list[0],\"-\",temp_split_list[1]])\n",
    "            tokenized_label_list2.extend([tokenized_label_list[i]]*3)\n",
    "        elif len(temp_split_list) == 3:\n",
    "            tokenized_word_list2.extend([temp_split_list[0],\"-\",temp_split_list[1],\"-\",temp_split_list[2]])\n",
    "            tokenized_label_list2.extend([tokenized_label_list[i]]*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_word_list = []\n",
    "tokenized_label_list =  []\n",
    "\n",
    "for i in tqdm(range(len(tokenized_word_list2))):\n",
    "    if tokenized_word_list2[i] != \"\":\n",
    "        tokenized_word_list.append(tokenized_word_list2[i])\n",
    "        tokenized_label_list.append(tokenized_label_list2[i])\n",
    "\n",
    "tokenized_word_list2 = []\n",
    "tokenized_label_list2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making senetnces of length 8 which will be input to the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(word_list, label_list, l):\n",
    "    sentences = []\n",
    "    sentence_labels = []\n",
    "    num_of_words = len(word_list)\n",
    "    for i in tqdm(range(0,num_of_words,l)):\n",
    "        if i+l < num_of_words:\n",
    "            sentences.append(word_list[i:i+l])\n",
    "            sentence_labels.append(label_list[i:i+l])\n",
    "        else:\n",
    "            sentences.append(word_list[i:])\n",
    "            sentence_labels.append(label_list[i:])\n",
    "    return sentences, sentence_labels\n",
    "    \n",
    "l = 8\n",
    "sentences, sentence_labels = get_sentences(tokenized_word_list, tokenized_label_list, l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sentences, open( \"sentences_8.p\", \"wb\" ))\n",
    "pickle.dump(sentence_labels, open( \"sentence_labels_8.p\", \"wb\" ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
